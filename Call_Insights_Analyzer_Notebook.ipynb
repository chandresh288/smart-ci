{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc46852",
   "metadata": {},
   "source": [
    "#  Smart Call Insights Analyzer\n",
    "\n",
    "This notebook demonstrates an end-to-end workflow to **transcribe customer support calls**, extract structured insights using **OpenAI's GPT API**, and generate **visual reports and explanations**. The use case is focused on customer queries related to **CCTV camera setup and configuration**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Features Covered\n",
    "\n",
    "-  Transcribe audio files using **Whisper ASR**\n",
    "-  Extract insights like **summary, topics, action items, sentiment**\n",
    "-  Visualize trends using **pandas + matplotlib**\n",
    "-  Explain predictions using **SHAP**\n",
    "-  Suitable for AWS-based automation pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c94fa",
   "metadata": {},
   "source": [
    "##  Step 1: Setup Configuration\n",
    "\n",
    "Define input/output directories, load Whisper ASR model, and configure OpenAI API access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c11eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import whisper\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from openai import OpenAI\n",
    "\n",
    "AUDIO_DIR    = \"calls_audio_2/\"\n",
    "OUTPUT_DIR   = \"calls_output/\"\n",
    "WHISPER_MODEL = \"medium\"\n",
    "MAX_WORKERS  = 4\n",
    "\n",
    "client = OpenAI(api_key=\"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\")  # Replace with your actual key\n",
    "\n",
    "whisper_model = whisper.load_model(WHISPER_MODEL)\n",
    "\n",
    "def transcribe_file(audio_path: str) -> str:\n",
    "    result = whisper_model.transcribe(audio_path)\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e4b96",
   "metadata": {},
   "source": [
    "##  Step 2: Transcription with Whisper\n",
    "\n",
    "Use OpenAI's Whisper ASR (speech-to-text) to convert `.mp3` or `.wav` audio files into transcripts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3a308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSIGHT_PROMPT = \"\"\"\n",
    "You are an AI assistant that reads a call transcript and extracts:\n",
    "1. A concise summary (2–3 sentences).\n",
    "2. Key topics covered.\n",
    "3. Action items (who needs to do what by when, if mentioned).\n",
    "4. Overall sentiment/tone of the call.\n",
    "Provide output as a JSON with keys: summary, topics, action_items, sentiment.\n",
    "Transcript:\n",
    "\\\"\\\"\\\"\n",
    "{transcript}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def extract_insights(transcript: str) -> dict:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You extract structured call insights as JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": INSIGHT_PROMPT.format(transcript=transcript)}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content.strip()\n",
    "    try:\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Could not parse JSON\", \"raw\": content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fb1a3",
   "metadata": {},
   "source": [
    "##  Step 4: Full Pipeline Execution\n",
    "\n",
    "This function processes all calls in the folder by:\n",
    "1. Transcribing the call\n",
    "2. Extracting insights\n",
    "3. Saving results as `.txt` and `.json` files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d58d98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AUDIO_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m#     futures = {executor.submit(process_call, path): path for path in audio_files}\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#     for future in as_completed(futures):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m#         except Exception as exc:\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m#             print(f\"[Error] {path}: {exc!r}\")\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_all.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+a\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     54\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(dataset, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m     audio_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mAUDIO_DIR\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.*\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(audio_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m audio files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAUDIO_DIR\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     dataset_all \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AUDIO_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import whisper\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from openai import OpenAI\n",
    "\n",
    "def process_call(audio_path: str):\n",
    "    fname = Path(audio_path).stem\n",
    "    out_folder = Path(OUTPUT_DIR) / fname\n",
    "    out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    transcript = transcribe_file(audio_path)\n",
    "    txt_path = out_folder / f\"{fname}.txt\"\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcript)\n",
    "\n",
    "    insights = extract_insights(transcript)\n",
    "    json_path = out_folder / f\"{fname}_insights.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(insights, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[Done] {fname} → transcript + insights\")\n",
    "    return fname\n",
    "\n",
    "def main():\n",
    "    audio_files = glob.glob(os.path.join(AUDIO_DIR, \"*.*\"))\n",
    "    print(f\"Found {len(audio_files)} audio files in {AUDIO_DIR!r}\")\n",
    "    dataset_all = []\n",
    "    for path in audio_files:\n",
    "        fname = Path(path).stem\n",
    "        out_folder = Path(OUTPUT_DIR) / fname\n",
    "        txt_path = out_folder / f\"{fname}.txt\"\n",
    "        with open(txt_path, encoding=\"utf-8\") as f:\n",
    "            dataset_all.append({\n",
    "                \"call_id\": fname,\n",
    "                \"transcript\": f.read()\n",
    "            })\n",
    "    return dataset_all\n",
    "\n",
    "    # with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    #     futures = {executor.submit(process_call, path): path for path in audio_files}\n",
    "    #     for future in as_completed(futures):\n",
    "    #         path = futures[future]\n",
    "    #         try:\n",
    "    #             future.result()\n",
    "    #         except Exception as exc:\n",
    "    #             print(f\"[Error] {path}: {exc!r}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = main()\n",
    "    with open(\"dataset_all.json\",\"+a\") as file:\n",
    "        file.write(json.dumps(dataset, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe588c1",
   "metadata": {},
   "source": [
    "##  Step 5: Flatten & Copy Audio Files\n",
    "\n",
    "Copy all `.mp3` audio files from nested folders to a flat directory for easy batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfe013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "SRC_DIR = Path(\"calls_audio\")\n",
    "DEST_DIR = Path(\"calls_audio_2\")\n",
    "DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for mp3_path in SRC_DIR.rglob(\"*.mp3\"):\n",
    "    dest_path = DEST_DIR / mp3_path.name\n",
    "    shutil.copy2(mp3_path, dest_path)\n",
    "    print(f\"Copied: {mp3_path} → {dest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778a936",
   "metadata": {},
   "source": [
    "##  Step 6: Data Visualization\n",
    "\n",
    "Visualize:\n",
    "- Sentiment distribution\n",
    "- Action item frequency\n",
    "- Top 10 topics from the call data\n",
    "\n",
    "Useful for stakeholder reporting and call center analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc27e33",
   "metadata": {},
   "source": [
    "##  Step 7: Summary Reporting\n",
    "\n",
    "Generate structured tables:\n",
    "- Sentiment Summary\n",
    "- Action Items Summary\n",
    "- Topic Frequency\n",
    "- Overall Call Health (missing insights)\n",
    "\n",
    "Exports as CSVs for integration with dashboards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b10893",
   "metadata": {},
   "source": [
    "##  Step 8: Explainability with SHAP\n",
    "\n",
    "Use a surrogate classifier (Logistic Regression on TF-IDF) to simulate how sentiment is predicted. Visualize using SHAP to identify **important keywords** influencing each prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f50c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  You're Ready to Analyze Support Calls!\n",
    "\n",
    "This notebook can be deployed locally or on cloud environments (e.g., SageMaker, EC2, Lambda + Glue).\n",
    "\n",
    "Make sure to:\n",
    "- Replace `sk-proj-xxx` with your actual OpenAI API key\n",
    "- Provide `.mp3` audio files under `calls_audio/`\n",
    "\n",
    "For production-scale use, this can be automated via AWS Glue and orchestrated using CloudFormation or Airflow.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
